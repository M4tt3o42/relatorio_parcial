{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fullpistoladev/relatorio_parcial/blob/main/Pibic_relat%C3%B3rio_parcial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gWapBzpTpqe",
        "outputId": "3b88e767-b7db-46ab-913e-d784e63bdea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHfk4eE8TvlM",
        "outputId": "1f53eb94-22fd-4e13-f6eb-e803ad793ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-10 17:34:14.615131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-10 17:34:15.788856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pt-core-news-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.5.0/pt_core_news_lg-3.5.0-py3-none-any.whl (568.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-lg==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.1.2)\n",
            "Installing collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download pt_core_news_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09P7rp0jUdMV",
        "outputId": "b2590c8d-4318-494c-9a1c-3336b4bdd4cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Stbt06TFVf1t"
      },
      "outputs": [],
      "source": [
        "\n",
        "def to_lowercase(data):\n",
        "    return [line.replace(line, line.lower()) for line in data]\n",
        "\n",
        "def remove_stopwords(data):\n",
        "    stop_words = spacy.lang.pt.stop_words.STOP_WORDS\n",
        "    filtered_tokens = []\n",
        "    \n",
        "    for line in data:\n",
        "        # tokenize the line with spaCy\n",
        "        doc = nlp(line)\n",
        "        \n",
        "        # remove stopwords from the doc\n",
        "        filtered_tokens_line = [token.text for token in doc if token.text.lower() not in stop_words]\n",
        "        \n",
        "        # join the remaining tokens back into a single string\n",
        "        filtered_line = ' '.join(filtered_tokens_line)\n",
        "        \n",
        "        filtered_tokens.append(filtered_line)\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "def remove_punct(data):\n",
        "    #regex para tudo que não for [A-Za-Z0-9] e espaçamento \n",
        "    return [re.sub(r'[^\\w\\s]', ' ', str(line)) for line in data]\n",
        "\n",
        "\n",
        "def lemmatize(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lem_tokens = []\n",
        "    for line in data:\n",
        "        tokens = word_tokenize(line, language='portuguese')\n",
        "        lem_line = []\n",
        "        for token in tokens:\n",
        "            lem_token = lemmatizer.lemmatize(token)\n",
        "            lem_line.append(lem_token)\n",
        "        lem_string = ' '.join(lem_line)\n",
        "        lem_tokens.append(lem_string)\n",
        "    return lem_tokens\n",
        "\n",
        "def stem(data):\n",
        "    stemmer = SnowballStemmer('portuguese')\n",
        "    stem_tokens = []\n",
        "    for line in data:\n",
        "        tokens = word_tokenize(line, language='portuguese')\n",
        "        stem_line = []\n",
        "        for token in tokens:\n",
        "            stem_token = stemmer.stem(token)\n",
        "            stem_line.append(stem_token)\n",
        "        stem_string = ' '.join(stem_line)\n",
        "        stem_tokens.append(stem_string)\n",
        "    return stem_tokens\n",
        "\n",
        "def preprocess_text(data):\n",
        "    \n",
        "    data = remove_punct(data)\n",
        "    data = to_lowercase(data)\n",
        "    data = remove_stopwords(data)\n",
        "    data = lemmatize(data)\n",
        "    data = stem(data)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEkN2hN-WA0V"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('pt_core_news_lg') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWz2aT7HVqZ3",
        "outputId": "783a95df-e666-4ffd-f7e1-9b9ecf1031ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          questaoMatrizId                  nomeCandidato  \\\n",
            "0    0b991be1-84ab-40af-8d4b-94d4aa0ac9b2      ANDREON SOUZA DE MEDEIROS   \n",
            "1    83099bc5-0c76-4633-a6f2-f89d50c673aa      ANDREON SOUZA DE MEDEIROS   \n",
            "2    e39bcb05-5a01-4b72-81ca-049b438fce69      ANDREON SOUZA DE MEDEIROS   \n",
            "3    bc490271-4f7e-465a-8e8e-c1d1a5936612      ANDREON SOUZA DE MEDEIROS   \n",
            "4    0b991be1-84ab-40af-8d4b-94d4aa0ac9b2   ANNY KLARICE FERNANDES SOUZA   \n",
            "..                                    ...                            ...   \n",
            "119  bc490271-4f7e-465a-8e8e-c1d1a5936612   WESLLEY BERNARDO DE OLIVEIRA   \n",
            "120  0b991be1-84ab-40af-8d4b-94d4aa0ac9b2  YURI GUILHERME MOURA DE SOUZA   \n",
            "121  83099bc5-0c76-4633-a6f2-f89d50c673aa  YURI GUILHERME MOURA DE SOUZA   \n",
            "122  e39bcb05-5a01-4b72-81ca-049b438fce69  YURI GUILHERME MOURA DE SOUZA   \n",
            "123  bc490271-4f7e-465a-8e8e-c1d1a5936612  YURI GUILHERME MOURA DE SOUZA   \n",
            "\n",
            "                              candidatoId  \\\n",
            "0    1cf4ed20-e3bb-43d1-8901-ee3a70aefc05   \n",
            "1    1cf4ed20-e3bb-43d1-8901-ee3a70aefc05   \n",
            "2    1cf4ed20-e3bb-43d1-8901-ee3a70aefc05   \n",
            "3    1cf4ed20-e3bb-43d1-8901-ee3a70aefc05   \n",
            "4    df622f5a-194d-4bb0-87fc-ceeb404bbc9e   \n",
            "..                                    ...   \n",
            "119  0a0c9d3c-1db4-43fc-addf-fb147b609a27   \n",
            "120  8994848c-dfd5-42b7-a48a-e9c7bf5dcd62   \n",
            "121  8994848c-dfd5-42b7-a48a-e9c7bf5dcd62   \n",
            "122  8994848c-dfd5-42b7-a48a-e9c7bf5dcd62   \n",
            "123  8994848c-dfd5-42b7-a48a-e9c7bf5dcd62   \n",
            "\n",
            "                          discursiva.respostaCandidato  notaQuestao  \n",
            "0    Polimorfismo é, como o nome sugere (múltiplas ...         1.11  \n",
            "1    Classes são modelos/\"estruturas\" de coisas tra...         1.11  \n",
            "2    Herança, em POO, é bem fácil de ser entendida ...         1.00  \n",
            "3    Os possíveis modificadores de acesso são 4: pu...         1.11  \n",
            "4    O polimorfismo estático ele permite que nós cr...         0.40  \n",
            "..                                                 ...          ...  \n",
            "119                                                            0.00  \n",
            "120  Polimorfismo permite que a partir de um contra...         0.70  \n",
            "121  Classes são estruturas de dados, que reunem um...         0.60  \n",
            "122  Herança permite que atributos ou métodos de um...         1.11  \n",
            "123  public - Permite que qualquer método ou atribu...         0.75  \n",
            "\n",
            "[124 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "path_questions = \"drive/MyDrive/Colab Notebooks/data/questions.xlsx\"\n",
        "path_answers = \"drive/MyDrive/Colab Notebooks/data/answers.xlsx\"\n",
        "\n",
        "questions = pd.read_excel(path_questions)\n",
        "answers = pd.read_excel(path_answers)\n",
        "print(answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMtBLJf9WP-e",
        "outputId": "91b58631-6370-4d0b-cd60-5958bc74c91f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-645283dc7971>:18: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  similarity = expected_answers.similarity(student_answer)\n",
            "<ipython-input-7-645283dc7971>:18: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  similarity = expected_answers.similarity(student_answer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      0.887435\n",
            "4      0.805268\n",
            "8      0.277430\n",
            "12     0.928847\n",
            "16     0.898586\n",
            "         ...   \n",
            "107    0.584143\n",
            "111    0.661283\n",
            "115    0.686739\n",
            "119    0.000000\n",
            "123    0.719466\n",
            "Name: wordembedding_similarity, Length: 113, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-645283dc7971>:18: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  similarity = expected_answers.similarity(student_answer)\n"
          ]
        }
      ],
      "source": [
        "spacy_similarity = pd.DataFrame()\n",
        "\n",
        "\n",
        "for questionId in range(4):\n",
        "\n",
        "  temp = answers[answers['questaoMatrizId'] ==  questions['questaoMatrizId'][questionId]]\n",
        "  temp = temp.dropna(subset=['discursiva.respostaCandidato'])\n",
        "  expected_answers = nlp(questions['discursiva.expectativaDeResposta'][questionId])\n",
        "  #filtered_tokens = [token.text for token in expected_answers if not token.is_stop]\n",
        "  #expected_answers = nlp(' '.join(filtered_tokens))\n",
        "\n",
        "  for index, row in temp.iterrows():\n",
        "    #print(row)\n",
        "    \n",
        "    student_answer = nlp(row['discursiva.respostaCandidato'])\n",
        "    #student_answer = [token.text for token in student_answer if not token.is_stop]\n",
        "    #student_answer_filtered_tokens = nlp(' '.join(student_answer))\n",
        "    similarity = expected_answers.similarity(student_answer)\n",
        "    temp.loc[index, 'wordembedding_similarity'] = similarity\n",
        "  spacy_similarity = pd.concat([spacy_similarity, temp])\n",
        "    #print(f'{row.nomeCandidato} => {row.questaoMatrizId} => {row.notaQuestao} => {similarity}')\n",
        "\n",
        "\n",
        "print(spacy_similarity['wordembedding_similarity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_peH9sWKWYkt",
        "outputId": "cf79f6a9-1335-4cc9-ee6b-172ca6932ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['10' '20' 'args' 'argument' 'automovel' 'característ' 'cham' 'checklist'\n",
            " 'class' 'compil' 'dad' 'depend' 'deriv' 'diferent' 'dinâm' 'estát' 'exat'\n",
            " 'execut' 'extends' 'há' 'instânc' 'int' 'linguagens' 'main' 'mant'\n",
            " 'métod' 'new' 'objet' 'ocorr' 'oficin' 'op' 'op1' 'op2' 'oper' 'operat'\n",
            " 'operating' 'operation' 'orient' 'out' 'parâmetr' 'pass' 'permit'\n",
            " 'polimorf' 'possibilit' 'princip' 'println' 'process' 'program' 'public'\n",
            " 'quot' 'ser' 'sobrescrit' 'static' 'string' 'system' 'tip' 'two' 'veicul'\n",
            " 'void']\n",
            "[[0.60810142]]\n",
            "[[0.60093503]]\n",
            "[[0.29042347]]\n",
            "[[0.62678801]]\n",
            "[[0.64379571]]\n",
            "[[0.63108559]]\n",
            "[[0.67924863]]\n",
            "[[0.53096841]]\n",
            "[[0.34931068]]\n",
            "[[0.65601227]]\n",
            "[[0.67340316]]\n",
            "[[0.24040121]]\n",
            "[[0.54785657]]\n",
            "[[0.6767351]]\n",
            "[[0.23253408]]\n",
            "[[0.67376219]]\n",
            "[[0.27794816]]\n",
            "[[0.65845728]]\n",
            "[[0.59707503]]\n",
            "[[0.44910847]]\n",
            "[[0.31902304]]\n",
            "[[0.51664355]]\n",
            "[[0.62547526]]\n",
            "[[0.27907279]]\n",
            "[[0.54620612]]\n",
            "[[0.49193661]]\n",
            "['cham' 'class' 'cri' 'criaçã' 'defin' 'definiçã' 'descrit' 'funçã'\n",
            " 'instânc' 'linguag' 'materializ' 'model' 'objet' 'orient' 'poss'\n",
            " 'preocup' 'tip' 'utiliz']\n",
            "[[0.63466306]]\n",
            "[[0.6617241]]\n",
            "[[0.76752258]]\n",
            "[[0.66332496]]\n",
            "[[0.64749621]]\n",
            "[[0.68931235]]\n",
            "[[0.71350607]]\n",
            "[[0.66332496]]\n",
            "[[0.68055705]]\n",
            "[[0.6992059]]\n",
            "[[0.70655557]]\n",
            "[[0.63960215]]\n",
            "[[0.56918094]]\n",
            "[[0.62279916]]\n",
            "[[0.71914652]]\n",
            "[[0.67469908]]\n",
            "[[0.61157663]]\n",
            "[[0.62279916]]\n",
            "[[0.66149509]]\n",
            "[[0.6504925]]\n",
            "[[0.66057826]]\n",
            "[[0.67861848]]\n",
            "[[0.68224229]]\n",
            "[[0.70244329]]\n",
            "[[0.6504925]]\n",
            "[[0.7591341]]\n",
            "[[0.69114828]]\n",
            "[[0.]]\n",
            "[[0.67316183]]\n",
            "['absorv' 'atribut' 'bas' 'cham' 'class' 'conceit' 'códig' 'deriv'\n",
            " 'estrutur' 'extends' 'ger' 'heranc' 'herd' 'manuten' 'métod' 'permit'\n",
            " 'princip' 'public' 'reutiliz' 'sub' 'superior' 'vantagens' 'void']\n",
            "[[0.85347365]]\n",
            "[[0.81584549]]\n",
            "[[0.68516016]]\n",
            "[[0.72531852]]\n",
            "[[0.80029234]]\n",
            "[[0.77517023]]\n",
            "[[0.77992602]]\n",
            "[[0.79125657]]\n",
            "[[0.62636823]]\n",
            "[[0.69989647]]\n",
            "[[0.84327404]]\n",
            "[[0.76552396]]\n",
            "[[0.73835038]]\n",
            "[[0.82220642]]\n",
            "[[0.82753539]]\n",
            "[[0.81323592]]\n",
            "[[0.80664584]]\n",
            "[[0.67598966]]\n",
            "[[0.85596499]]\n",
            "[[0.84970583]]\n",
            "[[0.76488086]]\n",
            "[[0.82891112]]\n",
            "[[0.76696499]]\n",
            "[[0.82098966]]\n",
            "[[0.78902403]]\n",
            "[[0.76985903]]\n",
            "[[0.81082819]]\n",
            "[[0.]]\n",
            "[[0.80445045]]\n",
            "['acess' 'class' 'classepublic' 'default' 'deriv' 'gt' 'irrestrit' 'pacot'\n",
            " 'permit' 'privat' 'protected' 'própr']\n",
            "[[0.60252438]]\n",
            "[[0.66898551]]\n",
            "[[0.53721531]]\n",
            "[[0.56343617]]\n",
            "[[0.53452248]]\n",
            "[[0.65081403]]\n",
            "[[0.63968818]]\n",
            "[[0.62300896]]\n",
            "[[0.61779761]]\n",
            "[[0.62540178]]\n",
            "[[0.64168895]]\n",
            "[[0.65324247]]\n",
            "[[0.64930947]]\n",
            "[[0.69102332]]\n",
            "[[0.62994079]]\n",
            "[[0.64293062]]\n",
            "[[0.55301004]]\n",
            "[[0.59160798]]\n",
            "[[0.66045819]]\n",
            "[[0.62540178]]\n",
            "[[0.60894276]]\n",
            "[[0.5702248]]\n",
            "[[0.58338335]]\n",
            "[[0.59844375]]\n",
            "[[0.58191437]]\n",
            "[[0.50395263]]\n",
            "[[0.65324247]]\n",
            "[[0.]]\n",
            "[[0.55328334]]\n",
            "0      0.608101\n",
            "4      0.600935\n",
            "8      0.290423\n",
            "12     0.626788\n",
            "16     0.643796\n",
            "         ...   \n",
            "107    0.581914\n",
            "111    0.503953\n",
            "115    0.653242\n",
            "119    0.000000\n",
            "123    0.553283\n",
            "Name: tf_similarity, Length: 113, dtype: float64 \n",
            "\n",
            " 0      1.11\n",
            "4      0.40\n",
            "8      0.10\n",
            "12     0.40\n",
            "16     1.00\n",
            "       ... \n",
            "107    0.78\n",
            "111    0.40\n",
            "115    0.60\n",
            "119    0.00\n",
            "123    0.75\n",
            "Name: notaQuestao, Length: 113, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "tf_similarity = pd.DataFrame()\n",
        "\n",
        "for questionId in range(4):\n",
        "\n",
        "  temp = answers[answers['questaoMatrizId'] ==  questions['questaoMatrizId'][questionId]]\n",
        "  temp = temp.dropna(subset=['discursiva.respostaCandidato'])\n",
        "  expectativa_de_resposta = [questions['discursiva.expectativaDeResposta'][questionId]]\n",
        "  expectativa_de_resposta = preprocess_text(expectativa_de_resposta)\n",
        "  tf_matrix = vectorizer.fit_transform(expectativa_de_resposta)\n",
        "  print(vectorizer.get_feature_names_out())\n",
        "  \n",
        "  for index, row in temp.iterrows():\n",
        "    temp_answer = [temp['discursiva.respostaCandidato'][index]]\n",
        "    temp_answer = preprocess_text(temp_answer)\n",
        "    tf_answer = vectorizer.transform(temp_answer)\n",
        "    count_vectorizer_similarity = cosine_similarity(tf_answer, tf_matrix)\n",
        "    print(count_vectorizer_similarity)\n",
        "    temp.loc[index, 'tf_similarity'] = count_vectorizer_similarity[0]\n",
        "    # print(count_vectorizer_similarity[0])\n",
        "    #print(f'{row.nomeCandidato} => {row.questaoMatrizId} => {row.notaQuestao} => {count_vectorizer_similarity[0]}')\n",
        "  tf_similarity = pd.concat([tf_similarity, temp])\n",
        "print(tf_similarity['tf_similarity'],'\\n\\n', tf_similarity['notaQuestao'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiBGS7w6sdnQ",
        "outputId": "02837cc0-d06e-4c4d-f661-cfaf4bc6e5aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.1.0-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.7/152.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter\n",
        "import xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1oFBKiPr1iA"
      },
      "outputs": [],
      "source": [
        "writer = pd.ExcelWriter('drive/MyDrive/Colab Notebooks/data/similarities_sheet.xlsx', engine='xlsxwriter')\n",
        "\n",
        "# write the dataframes to different sheets of the same workbook\n",
        "tfidf_similarity.to_excel(writer, sheet_name='tfidf_similarity')\n",
        "tf_similarity.to_excel(writer, sheet_name='tf_similarity')\n",
        "spacy_similarity.to_excel(writer, sheet_name='spacy_similarity')\n",
        "\n",
        "# save and close the Excel writer object\n",
        "writer.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLziLJXBpxoG",
        "outputId": "38a02fec-52ca-42dd-fbaf-b31172ab10a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                          questaoMatrizId                   nomeCandidato  \\\n",
            "0    0b991be1-84ab-40af-8d4b-94d4aa0ac9b2       ANDREON SOUZA DE MEDEIROS   \n",
            "1    0b991be1-84ab-40af-8d4b-94d4aa0ac9b2    ANNY KLARICE FERNANDES SOUZA   \n",
            "2    0b991be1-84ab-40af-8d4b-94d4aa0ac9b2       BRUNO DA FONSECA DUMARESQ   \n",
            "3    0b991be1-84ab-40af-8d4b-94d4aa0ac9b2   DANYLLO ROBEMAR VILELA SANTOS   \n",
            "4    0b991be1-84ab-40af-8d4b-94d4aa0ac9b2  DANIEL OTAVIANO BEZERRA PONTES   \n",
            "..                                    ...                             ...   \n",
            "108  bc490271-4f7e-465a-8e8e-c1d1a5936612        VITORIA MARIA LUCIO DEON   \n",
            "109  bc490271-4f7e-465a-8e8e-c1d1a5936612          WALCKER DA SILVA GOMES   \n",
            "110  bc490271-4f7e-465a-8e8e-c1d1a5936612    WASHINGTON WAGNER DAVID CRUZ   \n",
            "111  bc490271-4f7e-465a-8e8e-c1d1a5936612    WESLLEY BERNARDO DE OLIVEIRA   \n",
            "112  bc490271-4f7e-465a-8e8e-c1d1a5936612   YURI GUILHERME MOURA DE SOUZA   \n",
            "\n",
            "                              candidatoId  \\\n",
            "0    1cf4ed20-e3bb-43d1-8901-ee3a70aefc05   \n",
            "1    df622f5a-194d-4bb0-87fc-ceeb404bbc9e   \n",
            "2    9aa55331-63de-414e-9d62-8bbd9f9ce3a7   \n",
            "3    16ffac93-6dfd-409b-8cc2-d8b6fabc3c5e   \n",
            "4    42756873-7dc9-4b04-9d9e-37bce1044452   \n",
            "..                                    ...   \n",
            "108  64d47120-60ed-4388-9d9e-6576b652a897   \n",
            "109  1570eebd-7234-4ae5-8a0a-19192eeed120   \n",
            "110  89c9de02-6b7e-4711-b006-60ada4afeb28   \n",
            "111  0a0c9d3c-1db4-43fc-addf-fb147b609a27   \n",
            "112  8994848c-dfd5-42b7-a48a-e9c7bf5dcd62   \n",
            "\n",
            "                          discursiva.respostaCandidato  notaQuestao  \\\n",
            "0    Polimorfismo é, como o nome sugere (múltiplas ...         1.11   \n",
            "1    O polimorfismo estático ele permite que nós cr...         0.40   \n",
            "2    Polimorfismo é o princípio ao qual as classes ...         0.10   \n",
            "3    É a capacidade que um mesmo tipo tem de assumi...         0.40   \n",
            "4    A primeiro categoria de polimorfismo é evidenc...         1.00   \n",
            "..                                                 ...          ...   \n",
            "108  Public: os atributos podem ser acessados por o...         0.78   \n",
            "109  public: esse modificador permite que os atribu...         0.40   \n",
            "110  Em Java existe a possibilidade de modificar a ...         0.60   \n",
            "111                                                            0.00   \n",
            "112  public - Permite que qualquer método ou atribu...         0.75   \n",
            "\n",
            "     tfidf_similarity  tf_similarity  wordembedding_similarity  \n",
            "0            0.608101       0.608101                  0.887435  \n",
            "1            0.600935       0.600935                  0.805268  \n",
            "2            0.290423       0.290423                  0.277430  \n",
            "3            0.626788       0.626788                  0.928847  \n",
            "4            0.643796       0.643796                  0.898586  \n",
            "..                ...            ...                       ...  \n",
            "108          0.581914       0.581914                  0.584143  \n",
            "109          0.503953       0.503953                  0.661283  \n",
            "110          0.653242       0.653242                  0.686739  \n",
            "111          0.000000       0.000000                  0.000000  \n",
            "112          0.553283       0.553283                  0.719466  \n",
            "\n",
            "[113 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "similarities_df = merged_df = pd.merge(pd.merge(tfidf_similarity, tf_similarity, on=['questaoMatrizId', 'nomeCandidato', 'candidatoId', 'discursiva.respostaCandidato', 'notaQuestao'], how='outer'), spacy_similarity, on=['questaoMatrizId', 'nomeCandidato', 'candidatoId', 'discursiva.respostaCandidato', 'notaQuestao'], how='outer')\n",
        "similarities_df.to_excel('drive/MyDrive/Colab Notebooks/data/similarities.xlsx', index=False)\n",
        "print(similarities_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1iZNzT-dbq0rqXq9_qGB_Omus-AAcUAUE",
      "authorship_tag": "ABX9TyPVvE756hiUlO6Pcd/zps5N",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}